---
title: "Deploying and testing an OSG Hosted CE via SLATE"
overview: Blog
published: false
permalink: blog/deploy-hosted-ce.html
attribution: The SLATE Team
layout: post
type: markdown
---

# Deploying an OSG HostedCE

## Prerequisites
The OSG HostedCE uses SSH to submit pilot jobs that will connect back to the
OSG central pool. In order for the HostedCE to work, you'll first need to
create a local service account for OSG to submit jobs. This should be done
according to whatever process you normally use to create accounts for users.

## Generating and storing the key
Once the account has been created, you'll want to create a new SSH key pair.
The private part of the key will be stored within SLATE, and the public part of
the key will be installed into `authorized_keys` file of the OSG user on your
cluster. To generate the key, you'll need to run the following on some machine
with OpenSSH installed:

	ssh-keygen -f osg-keypair

Note that you will need to make this key passphraseless, as the HostedCE
software will consume this key. Once you've created the key, you'll want to
store the public part of it (osg-keypair.pub) into the `authorized_keys` file
on the OSG account for your cluster. For example, if your OSG service account
is called `osg`, you'll want to append the contents of `osg-keypair.pub` to
`/home/osg/.ssh/authorized_keys`. 

The private part of the keypair will need to be stored in SLATE for use by the
CE. To do that:

	slate secret create ce-private-key --from-file=bosco.key=osg-keypair

Where `ce-private-key` will be the name of the secret, and `osg-keypair` is the
path to your private key (assumed to be the current working directory).

## Configuring the CE from SLATE
You'll want to download the application configuration template:

	slate app get-conf --dev osg-hosted-ce > hosted-ce.yaml

There are several things that you'll need to edit here. 

### Site Section
First, for the site section you'll want to put in appropriate values that will
ultimately map onto your OSG Topology entry. Let's take a look at this group
first:

	Site:
	  Group: OSG
	  Resource: IRISHEP-SSL-UCHICAGO
	  ResourceGroup: UCHICAGO
	  Sponsor: osg:100
	  Contact: Lincoln Bryant
	  ContactEmail: lincolnb@uchicago.edu
	  City: Chicago
	  Country: US
	  Latitude: 41.8781
	  Longitude: 87.6298
	
Group and Sponsor are safe to leave as defaults if you plan to support OSG
users with your CE. From the [OSG Resource Registration
Documentation](https://opensciencegrid.org/docs/common/registration/), here's
the definition of Resource and ResourceGroup:

| Level          | Definition |
| Resource Group | A logical grouping of resources at a site. Production and testing resources must be placed into separate Resource Groups. | 
| Resource       | A host belonging to a resource group that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts. A resource may provide more than one service.) |

These will ultimately get mapped into the [OSG
Topology](https://topology.opensciencegrid.org/). Following those, you'll need
to update the Contact and location information as appropriate. The contact
information will be used to reach you in case there are any problems with your
CE or site.

### Cluster Section
Next we'll go through the Cluster section, this section defines the batch
system (e.g. HTCondor, Slurm, PBS, etc) used at your site. The `Users` field is
safe to leave as "osg" if you support primarily OSG jobs. The endpoint value
should be of the form `osg-service-account@your-login-node.site.edu`. 

You will also need to plug in your batch system type. Accepted values are
`condor`, `pbs`, `slurm`, `sge`, and `lsf`. My batch system is `condor`, and
I'll start by allowing a maximum of only 10 jobs at my site, to make sure OSG
pilots are working successfully at a small scale. 

Finally you'll need to refer to the private key we stored in SLATE previously.
I had called mine `ce-private-key`, so my configuration ends up
looking like this:

	Cluster:
	  Users: osg
	  Endpoint: osguser@slate-micro-condor.slateci.io
	  PrivateKeySecret: ce-private-key
	  Batch: condor
	  MaxJobs: 10

### Storage Section
For the Storage section, you'll want to define the location where the OSG Worker Node Client is installed and the location of temp/scratch space for your workers. The HostedCE SLATE application installs this automatically to the home directory of your service account, so you can just put that here. For temp, I have no particular directory in mind, so I will just use `/tmp`. So for me, it would be:

	Storage:
	  GridDir: /home/osguser/osg-wn-client
	  WorkerNodeTemp: /tmp

### Subcluster Section
The subcluster section will be used for reporting information about workers at
your site. You can read more on the [OSG
Docs](https://opensciencegrid.org/docs/other/configuration-with-osg-configure/#subcluster-resource-entry).
For my cluster, I'll plug in the appropriate values for my workers:

	Subcluster:
	  Name: RIVER
	  NodeCount: 75
	  Memory: 262144	
	  CpusPerNode: 2
	  CoresPerNode: 48

### Squid Section
If you have a Squid service running, you can ensure that your workers will
communicate with your local squid here. I will point my cluster to the local
squid:

	Squid:
	  Enabled: True
	  Location: squid.grid.uchicago.edu

(It's also possible to launch a Squid service through SLATE, see
[here](https://portal.slateci.io/applications/osg-frontier-squid))

### CustomizationScript
Your site may need some additional parameters to be added to jobs, such as
`SBATCH` parameters (Slurm) or `Requirements` arguments (HTCondor). You can
insert anything needed via the `CustomizationScript` parameter. For example,
we'll add an `+AccountingGroup` to our OSG jobs:

```
CustomizationScript: |+
  #!/bin/sh
  echo "+AccontingGroup=osg"
```

## Finalizing the configuration

Now that we've gone through the sections line-by-line, let's look at our completed configuration:

```
Site:
  Group: OSG
  Resource: IRISHEP-SSL-UCHICAGO
  ResourceGroup: UCHICAGO
  Sponsor: osg:100
  Contact: Lincoln Bryant
  ContactEmail: lincolnb@uchicago.edu
  City: Chicago
  Country: US
  Latitude: 41.8781
  Longitude: 87.6298

Cluster:
  Users: osg
  Endpoint: osguser@slate-micro-condor.slateci.io
  PrivateKeySecret: lincolnb-bosco # maps to SLATE secret
  Batch: condor
  MaxJobs: 10

Storage:
  GridDir: /home/osguser/osg-wn-client
  WorkerNodeTemp: /tmp

Subcluster:
  Name: RIVER
  NodeCount: 75
  Memory: 262144
  CpusPerNode: 2
  CoresPerNode: 48

Squid:
  Enabled: True
  Location: squid.grid.uchicago.edu

CustomizationScript: |+
  #!/bin/sh
  echo "+AccontingGroup=osg"
```

Once we're happy with the configuration, we can ask SLATE to install it:

	slate app install --cluster uchicago-prod --group slate-dev osg-hosted-ce --dev --conf hostedce.yaml 



## Testing the HostedCE
